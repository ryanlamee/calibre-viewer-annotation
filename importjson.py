'''
import a json generated from nw-extractor


usage:
    python importjson.py $JSONFILE

    or

    python importjson.py $JSONFILE $EPUBFILE

if $EPUBFILE is specified and exists, this will attempt to
derive (Anchor) values as well, and save them inside the
`extras` field in the (Annotator) object.


as of now, $JSONFILE is an output generated by nw-extractor.  it should contain
a JSON Object whose keys are `highlight` strings downloaded from kindle. this
means the strings will contain things like HTML entities.
the JSON Object's values are JSON Objects that should contain a fully specified
Annotation object, which contains, in particular, the `quote` key. The `quote`
is derived from the `highlight` string but is preprocessed to have HTML entities
converted. See file:nw-extractor/index.html:clean_string() for details.

'''
import os

DRY_RUN = os.environ.get('DRY_RUN') != 'FALSE'

import sys
if len(sys.argv) < 2:
    print(__doc__)
    sys.exit()

import json
import sqlalchemy as sqla
# path hack... to bypass calibre import needed in the plugin
sys.path.append('ViewerAnnotationPlugin')
import annotator_model as AModel
import annotator_store as AStore
import datetime
import bookreader as bkr
import anchortext as at
import HTMLParser # for html entities
from pprint import pprint as pp
from fuzzywuzzy import fuzz

json_filepath = sys.argv[1].strip()


book = None
corpus = None
uri2key = {}
# check if should load the corpus
if len(sys.argv) == 3:
    ebook_filepath = sys.argv[2].strip()
    if os.path.exists(ebook_filepath):
        book = bkr.epub.open_epub(ebook_filepath)
        print('reading book: %s' % ebook_filepath)
        corpus = bkr.epub_to_corpus(book, as_dict=True)

        # build uri2key
        for key in book.opf.manifest.keys():
            if key not in corpus: continue
            item = book.get_item(key)
            epub_uri = 'epub://%s' % item.href.split('/')[-1]
            uri2key[epub_uri] = key

DSN = 'sqlite:///%s' % os.path.expanduser('~/ebook-viewer-annotation.db')
AStore.setup_database(DSN)

# TODO: move into Annotation model
def add_extra(self, dc):
    '''
    since `extra` is currently a json-serialized string,
    this convenience fn reads a (dict)dc and adds it to
    the serialization
    '''
    extra = json.loads(self.extras)
    extra.update(dc)
    self.extras = json.dumps(extra)
    return self.extras

input_data = json.loads(open(sys.argv[1]).read())
html_parser = HTMLParser.HTMLParser()

title_set = set(entry['title'] for entry in input_data.values())
print('unique titles found in input data:')
for title in title_set:
    print('  - %s' % title)

# cache existing annotations so we don't create duplicates
sql = sqla.select([AModel.Annotation.title, AModel.Annotation.quote, AModel.Annotation.id]) \
        .where(sqla.or_(
            *[AModel.Annotation.title == title for title in title_set]))

quote_cache = {}
for row in AStore.session.execute(sql).fetchall():
    quote_cache[(row['title'], row['quote'])] = row['id']

# preprocess to prune the cache, as we will do full fuzzy matching
# on the whole cache in a 2nd pass
remain = []
for pair in input_data.iteritems():
    quote_key, entry = pair

    unescaped_quote = html_parser.unescape(quote_key)
    # for debugging... this should never execute because
    # presumably entry['quote'] = entities.decode(quote_key)
    # where `entities` is npm('html-entities')::Entities
    if unescaped_quote != entry['quote']:
        print('>> key: %s' % quote_key)
        print('>> js : %s' % entry['quote'])
        print('>> py : %s' % unescaped_quote)
        sys.exit()
    cache_key = (entry['title'], quote_key)
    ann_id = quote_cache.get(cache_key)
    if ann_id is not None:
        # pop it so fuzzy search doesn't try to use it later
        quote_cache.pop(cache_key)
    else:
        remain.append(pair)

print('%s of %s assumed already existing and skipped' % (len(input_data)-len(remain), len(input_data)))

MATCH_PERCENT_THRESHOLD = 80

nadded = 0
for i, (quote_key, entry) in enumerate(remain, start=1):
    print('processing %s of %s' % (i, len(remain)))
    d = entry.copy()
    # 'note' on kindle maps to 'text' in Annotator
    d['text'] = d.pop('note', None)
    d.update({
        'created': entry.get('created') \
                and datetime.datetime.fromtimestamp(entry['created']/1000)
                or datetime.datetime.now(),
        'user': entry.get('user') or AStore.CURRENT_USER_ID,
    })

    cache_key = (entry['title'], quote_key)
    is_found = False
    # fuzzy search
    for saved_pair in quote_cache:
        saved_title, saved_quote = saved_pair
        if saved_title != entry['title']:
            continue
        if fuzz.ratio(saved_quote, quote_key) > MATCH_PERCENT_THRESHOLD:
            print 'found match!'
            print '    O:',quote_key
            print '    N:',saved_quote
            is_found = True
            quote_cache.pop(saved_pair)
            break
    if is_found:
        continue

    print '='*60
    print 'NOT IN CURRENT DB, ADDING: ', quote_key

    ann = AModel.Annotation()
    ann.from_dict(d)

    corpus_key = uri2key.get(ann.uri)
    if corpus_key:
        anc = at.make_anchor(
                ann.quote,
                ann.ranges[0].startOffset,
                corpus_key,
                corpus,
                )
        add_extra(ann, dict(anchor = anc.to_dict()))

    nadded += 1
    if not DRY_RUN:
        AStore.session.add(ann)

if nadded > 0 and not DRY_RUN:
    AStore.session.commit()

print('-'*40)
print('final report: %s added.' % nadded)
if DRY_RUN:
    print('DRY RUN FINISHED')
else:
    print('APPLIED')

# remainder
pp(quote_cache)
