<!DOCTYPE HTML>
<html>
<head>
    <title>reconcile shtuf.</title>

    <script type="text/javascript" src="node_modules/xpath-range/node_modules/jquery/dist/cdn/jquery.js"></script>
    <script type="text/javascript" src="node_modules/js-yaml/dist/js-yaml.js"></script>

<script>
/**
 * TODO
 * should capture + display selection from child window for final reconcilation/manual fix
 */
var CWD = process.cwd();
var gui = require("nw.gui");
// webserver GLOBAL
var server;
var server_port = 3000;
function EXIT(code) {
    if(server) {
        server.close();
    }
    gui.App.quit();
    process.exit(code || 0);
}
// was hoping this would do cleanup,
// but this is not getting called now
gui.App.on("close", function() {
    EXIT();
});

USAGE = "\
\n\
########################################\n\
\n\
USAGE:\n\
\n\
nw . PATH_TO_BOOK.epub PATH_TO_HIGHLIGHTS.yml\n\
\n\
########################################\n\
\n\
\n\
";
if(gui.App.argv.length < 2) {
    process.stdout.write("\n");
    process.stdout.write("    need 2 arguments:\n");
    process.stdout.write("    - path-to-epub\n");
    process.stdout.write("    - path-to-annotation-yml\n");
    process.stdout.write("\n");
    process.stdout.write(USAGE);
    EXIT();
}

fs = require('fs');
path = require('path');
// we need to run a webserver in parallel unless you can figure out how
// to open a child window with an html string (read from the epub); the
// alternative is to extract the epub, which we are trying to avoid!
http = require('http');
EPub = require('epub');
Entities = require('html-entities').AllHtmlEntities;

var EPUB_PATH = gui.App.argv[0];
var YML_SRC_PATH = gui.App.argv[1];

var should_not_proceed = false;
if(!fs.existsSync(EPUB_PATH)) {
    process.stdout.write("\nsupplied book does not exist!\n");
    should_not_proceed = true;
}
if(!fs.existsSync(YML_SRC_PATH)) {
    process.stdout.write("\nsupplied yml file does not exist!\n");
    should_not_proceed = true;
}
if(should_not_proceed) {
    EXIT();
}

// Get the current window
var main_window = gui.Window.get();
// main_window.setPosition("center");
main_window.x = 200;
main_window.y = 200;
main_window.width = 1000;
main_window.height = 600;
// main_window.toggleFullscreen();

main_window.focus();

// main_window.showDevTools() 
window.addEventListener('keydown', function(evt) {
    if(evt.ctrlKey && evt.keyCode == 81) { // q
        EXIT();
    }
});

process.stdout.write("\n");
process.stdout.write(">>> epub file: " + EPUB_PATH + "\n");
process.stdout.write(">>> annotation yml filepath: " + YML_SRC_PATH);
process.stdout.write("\n\n");

var epub = new EPub(EPUB_PATH, "imagewebroot_ignore", "chapterwebroot_ignore");
</script>
<style>
td.yes-found {
    background-color: lime;
}
td.not-found {
    background-color: maroon;
}
table {
    font-size: small;
}

</style>

</head>
<body>

<div>
    click the "check" button to view where this passage was detected.
    click the "RECONCILE ALL" button at the bottom to search through all highlights to find matches and output the result file.
</div>
<div id="status"></div>
<table border="1"></table>

    <script type="text/javascript" charset="utf-8">
        $(document).ready(function() {


entities = new Entities();

// read highlights file
yml = jsyaml.safeLoad(fs.readFileSync(YML_SRC_PATH, "utf-8"));
yml.highlight_list.sort(function(a,b){return a.startLocation-b.startLocation});

/**
 * strategy:
   1. find all files that exist from the exploded epub
   2. first pass to build up candidate mapping of highlight_text -> part_file
      to process in 2nd pass. store the stuff that we can't find in a
      separate list and display that for manual marking
   3. second pass through the html list and actually mark the positions
      in range/cfi components

   it is probably possible to do this in 1 pass.  but
   since we don't know the correct kindle
   location->calibre page mapping, and there may be
   other position formats as well, it might be better
   to just keep it 2-pass; first pass to establish
   ranges and second to derive anchors.
 */

// 1. get list of files from exploded epub, cache text
function make_result(index, score, source_file) {
    return {
          index: index
        , score: score
        , source: source_file
    };
}

function clean_string(s) {
    return entities.decode(s.replace(/\s+$/, "").replace(/^\s+/, ""));
}

/**
 * check each token in token_list for presence in corpus.
 * order of appearance in corpus must follow order in token_list
 */
// a quick and dirty way to assess whether we should
// assume a match is found. corpus is assumed to be
// a fair bit longer than token_list. we want to see
// whether most of `token_list` exists in `corpus`.
// A better method would be to first extract all text
// in corpus, and minify it (remove extraneous spaces
// and symbols), then get something like levenshtein
// distance.
function get_corpus_coverage(corpus, token_list) {
    var hit_count = 0;
    for(var i = 0, offset = 0; i < token_list.length; ++i) {
        var token = token_list[i];
        var found_index = corpus.substr(offset).indexOf(token);
        if(found_index > -1) {
            hit_count++;
            offset += found_index;
        }
    }
    return hit_count;
}

// global result cache
n_active_search = 0;
output = {};

// 3. second pass
function launch_and_find(source_filepath, entry, close_after_ms) {
    var search_window = gui.Window.open("http://127.0.0.1:"+server_port+"/"+source_filepath, {
        "inject-js-start": "node_modules/xpath-range/xpath-range-bundle.js",
        "inject-js-end": "js/driver.js",
        position: "mouse"
    });
    n_active_search++;
    search_window.on("loaded", function() {
        // this console is still the parent's console
        // console.log("LOADED!!!")

        // this is the spawn's console
        // search_window.window.console.log("LOADED!!!");

        // repeat fuzzy find; the target's presence SHOULD be
        // guaranteed at this point
        var child_window = search_window.window;
        var search_string = clean_string(entry.highlight);
        var token_list = search_string.split(/\s+/);

        // to store the final result for writing to db!
        var annotation_object = null;

        if(child_window.find(search_string)) {
            // direct match
        } else {
            var sel = child_window.getSelection();
            // repeatedly search for longest substring, truncating from end
            var search_length = search_string.length;
            // 1 is nonsensical, but since the corpus is pre-filtered we expect
            // it to find it well before that!
            while(search_length-- > 1) {
                if(child_window.find(search_string.substr(0, search_length))) {
                    console.log("breaking from start find on iter: " + search_length);
                    break;
                }
            }
            // found longest start match, record for merging later
            var rng = child_window.getSelection().getRangeAt(0);
            var startNode = rng.startContainer;
            var startOffset = rng.startOffset;
            sel.removeAllRanges();

            // repeatedly search for longest substring, truncating from start
            var start_index = 1;
            while(start_index++ < search_string.length) {
                if(child_window.find(search_string.substr(start_index))) {
                    console.log("breaking from end find on iter: " + start_index);
                    break;
                }
            }
            var rng = child_window.getSelection().getRangeAt(0);
            var endNode = rng.endContainer;
            var endOffset = rng.endOffset;
            sel.removeAllRanges();

            var final_range = document.createRange();
            final_range.setStart(startNode, startOffset);
            final_range.setEnd(endNode, endOffset);
            sel.addRange(final_range);

        }

        // again, we are assuming the match has succeeded!
        var sel = child_window.getSelection(),
            verification_el = $("#td-sel-"+entry.startLocation+"-"+entry.endLocation),
            verification_string = sel.toString(),
            verification_token_list = verification_string.split(/\s+/),
            coverage_hit_count = get_corpus_coverage(entry.highlight, verification_token_list),
            coverage_score = coverage_hit_count / verification_token_list.length;
        verification_el.html(verification_string);
        var r = g = b = 0;
        if(coverage_score > 0.5) {
            r = Math.floor(256 * (1-(coverage_score-0.5)*2));
            g = 256;
        } else {
            r = 256;
            g = Math.floor(256 * (1-(0.5-coverage_score)*2));
        }
        verification_el.css({backgroundColor: "rgb("+Math.floor(r)+","+Math.floor(g)+","+b+")"});
        sel.anchorNode.parentNode.scrollIntoView();
        sRange = child_window.doMagic();

        if(close_after_ms != null) {
            // NOTE: it turns out annotator manipulates the dom.
            // so we need to apply its manipulation prior reading the cfi for calibre
            // this also means that if you run this with the "check" button (window
            // doesn't auto close), and capture the cfi from here, it WILL NOT WORK
            // in calibre
            child_window.$(child_window.document.body).annotator();
        }

        // THIS IS WHAT WE'VE BEEN WAITING FOR!
        annotation_object = {
            uri: "epub://"+source_filepath.split(path.sep).pop(),
            title: epub.metadata.title,
            created: entry.timestamp,
            quote: search_string,
            "ranges": [
                sRange.toObject()
            ],
            // append the cfi for Annotation Store
            "extras": {
                calibre_bookmark: {
                    type: "cfi",
                    /*
                       NOTE: we are stripping off the trailing parts of the cfi here.
                       i.e., "/2/4/2/140/1:2" becomes "/2/4/2/140"
                       
                       this is because when we save the cfi in the child window, we're saving
                       the cfi based on the currently rendered DOM without any annotation
                       elements applied. But when the annotation object is applied from the
                       database, the DOM is thus altered and we won't be able to scroll
                       reliably to the annotation position.
                       
                       based on limited testing I have encountered 2 cases:
                       1. a highlighted word within a <p></p> block. Saving cfi.at_current()
                          works. But then,
                       2. if an entire <p></p> block is highlighted, cfi.at_current()
                          subsequently fails due to the <p>... plain text ...</p> becoming
                          <p><span class="annotator-hl">... plain text ...</span></p> and the
                          original cfi no longer finding a match.
                       
                       The naive solution was to replace e.g. "/2/4/2/140/1:2" with
                       "/2/4/2/140/2/1:2", forcing all cfi's to assume an additional child,
                       but this makes case #1 above fail when the highlight is somewhere
                       inside the paragraph.
                       
                       Instead, then, we're going to assume we can simply ask calibre to
                       scroll to the parent element. I didn't read whether this conforms with
                       the cfi spec's expectations, but it seems to work so far.
                     */
                    pos: child_window.cfi.at_current().replace(new RegExp("([^/]+)$"), ""), // or manipulate the last fragment if you wish...
                    spine: spine_map[source_filepath]
                }
            }
        }
        if(annotation_object) {
            output[entry.highlight] = annotation_object;
            if(close_after_ms != null) {
                search_window.on("close", function() {
                    setTimeout(function() {
                        // console.log("closing...");
                        // really close; without true will infinite loop
                        // see https://github.com/rogerwang/node-webkit/wiki/Window
                        search_window.close(true);
                    }, close_after_ms);
                    n_active_search--;
                });
                search_window.close();
            }
        }
        child_window.addEventListener("keydown", function(evt) {
            // capture Ctrl-W to close window
            if(evt.keyCode == 87 && evt.ctrlKey) {
                search_window.close(true);
            }
        });
    });

}

/**
 * globals for caching useful data.
 * TODO XXX not sure about spine_map yet --
 * it's supposed to correspond to EbookViewer.iterator.spine.
 * i'm guessing it has length of N extracted html pages + 1
 * where +1 is index 0, or the cover. hence we ignore index 0
 */
var spine_map = {};
var html_map = {};
var annot_map = {};
var failure_list = [];

console.log(epub);
console.log(epub.manifest);
epub.on("end", function() {

    // build a entry list sorted in chapter order
    // array of manifest's `publication resources`
    var arr_pres = [];
    Object.keys(epub.manifest).forEach(function(key) {
        var pres = epub.manifest[key];
        if(!pres.href.match(/\.htm.+$/)) {
            return;
        }
        arr_pres.push(pres);
    });
    arr_pres.sort(function(a, b) {
        return a.href > b.href;
    });
    console.log("finished processing epub manifest, length: " + arr_pres.length);

    function populate_entry_by_index(entry_index) {
        var pres = arr_pres[entry_index];
        // console.log("proc arr pres... " + entry_index + " " + pres.id + ", " + pres.href);
        // note we used to do
        // html_map[filepath] = fs.readFileSync(filepath, "utf-8");
        // which is now replaced by the `getFile` block
        // this is now implicit because `epub` only supports utf-8
        // (see https://github.com/julien-c/epub)
        epub.getFile(pres.id, function(err, data, mimetype) {
            // console.log("read: " + data.length + " bytes from " + pres.href);
            html_map[pres.href] = data.toString('utf8');
            spine_map[pres.href] = entry_index+1;
            // console.log(pres.href + " done, now length: " + Object.keys(spine_map).length);
            if(Object.keys(html_map).length == arr_pres.length) {
                console.log("OK, READY.");
                build_candidate_mapping();
            }
        });
    }

    for(var entry_index=0; entry_index<arr_pres.length; ++entry_index) {
        populate_entry_by_index(entry_index);
    }

    function setup_server() {
        console.log("starting server...");
        server = http.createServer(function(req, resp) {
            // strip the leading /slash
            var filepath_candidate = req.url.substring(1);
            if(html_map[filepath_candidate]) {
                resp.writeHead(200, {"Content-Type": "text/html"});
                resp.end(html_map[filepath_candidate]);
            } else {
                resp.writeHead(404, {"Content-Type": "text/plain"});
                resp.end("not found");
            }
        }).listen(server_port, "127.0.0.1");

    // 2. build candidate mapping
    function build_candidate_mapping() {
        console.log("BUILDING CANDIDATE MAPPING...");
        setup_server();

    var file_start_index = 0;
    yml.highlight_list.forEach(function(entry) {
        var search_string = clean_string(entry.highlight);
        // port of "fuzzy search" see processkinde.py

        // since both the file list and annotation list are sorted, in every
        // annotation entry iteration we will exclude all files processed prior
        // to the latest match
        var file_index = file_start_index;
        for(var file_index = file_start_index; file_index<arr_pres.length; ++file_index) {
            var pres = arr_pres[file_index];
            var corpus = html_map[pres.href];
            var token_list = search_string.split(/\s+/);
            if(token_list.length < 4) {
                // direct match
                var loc = corpus.indexOf(search_string)
                if(loc > -1) {
                    annot_map[entry.highlight] = make_result(loc, 1.0, pres.href);
                    file_start_index = file_index;
                    break;
                }
            } else {
                var check_length = search_string.length*2; var offset = 0;
                var safety = 1000;
                var substring = null;
                while(offset < corpus.length) {
                    if(safety-- < 0) {
                        console.log("OUT OF CONTROL!!!! ", offset, check_length, corpus.length);
                        console.log("OUT OF CONTROL!!!! ", offset, check_length, corpus.length);
                        console.log("OUT OF CONTROL!!!! ", offset, check_length, corpus.length);
                        console.log("OUT OF CONTROL!!!! ", offset, check_length, corpus.length);
                        console.log(search_string, substring);
                        return;
                    }
                    if(corpus.substr(offset).indexOf(token_list[0]) == -1) {
                        break;
                    }

                    var found_index = offset + corpus.substr(offset).indexOf(token_list[0]);
                    substring = corpus.substr(found_index, check_length);
                    if(substring.indexOf(token_list[token_list.length-1]) == -1) {
                        offset = found_index+token_list[0].length;
                        continue;
                    }

                    // at this point, the first token check is redundant, but who cares
                    var hit_count = get_corpus_coverage(substring, token_list);
                    var score = hit_count/token_list.length;
                    if(score > 0.7) {
                        // console.log("FOUND! with score "+score);
                        annot_map[entry.highlight] = make_result(offset, score, pres.href);
                        file_start_index = file_index;
                        break;
                    } else {
                        offset = found_index+token_list[0].length;
                    }
                }
            }
        }
    });

    yml.highlight_list.forEach(function(entry) {
        if(!annot_map[entry.highlight]) {
            failure_list.push(entry.highlight);
        }
    });

    $("#status").html(Object.keys(annot_map).length + " found, " + failure_list.length + " failures ("+yml.highlight_list.length+" total)");

    // results display
    var tbl = $("table");
    var key_list = ["check?", "timestamp", "location", "found?", "highlight", "note", "verification"];
    var monster_list = [];
    yml.highlight_list.forEach(function(entry) {
        if(tbl.children().length == 0) {
            var tr = $("<tr>").appendTo(tbl);
            key_list.forEach(function(key) {
                $("<th>").html(key).appendTo(tr);
            });
        }

        var tr = $("<tr>");
        var res = annot_map[entry.highlight];
        key_list.forEach(function(key) {
            var td = $("<td>");
            switch(key) {
            case "check?":
                var test_button = $("<button>").html("check");
                td.append(test_button).appendTo(tr);
                test_button.click(function() {
                    launch_and_find(res.source, entry);
                });
                break;
            case "timestamp":
                td.html(new Date(entry[key]));
                break;
            case "location":
                td.html(entry.startLocation + "~" + entry.endLocation + " (" +(entry.endLocation-entry.startLocation)+ ")");
                break;
            case "found?":
                if(annot_map[entry.highlight]) {
                    var spl = res.source.split(path.sep);
                    td.html(res.score.toFixed(3) + " " + spl[spl.length-1]);
                    td.attr({class: "yes-found"});

                    monster_list.push([res.source, entry]);
                } else {
                    td.attr({class: "not-found"});
                }
                break;
            case "verification":
                // leave blank on init
                td.attr("id", "td-sel-"+entry.startLocation+"-"+entry.endLocation);
                break;
            default:
                if(entry[key]) {
                    td.html(entry[key]).appendTo(tr);
                }
            }
            td.appendTo(tr);
        });
        tr.appendTo(tbl);
    });

    $("<button>").html("RECONCILE ALL").click(function() {
        var interval_time = 150;
        var monster_index = 0;
        var monster_timer = setInterval(function() {
            process.stdout.write(">>> STATUS: " + Object.keys(output).length + " processed.\n");
            if(monster_index < monster_list.length) {
                var pair = monster_list[monster_index];
                monster_index++;
                // process.stdout.write("processing from: "+ pair[1] + "\n");
                launch_and_find(pair[0], pair[1], interval_time);
            } else {
                if(n_active_search == 0) {
                    process.stdout.write("ALL DONE :D\n");
                    clearInterval(monster_timer);

                    process.stdout.write("write to /tmp/out.json\n");
                    fs.writeFile("/tmp/out.json", JSON.stringify(output, null, 2));
                }
            }
        }, interval_time);
    }).insertAfter(tbl);

    } // build_candidate_mapping


});
epub.parse();
        }); // end document.ready

    </script>
</body>
</html>
